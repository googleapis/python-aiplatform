# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Code generated by the Google Gen AI SDK generator DO NOT EDIT.

import logging
from typing import Any, Optional, Union
from urllib.parse import urlencode

from google.genai import _api_module
from google.genai import _common
from google.genai._api_client import BaseApiClient
from google.genai._common import get_value_by_path as getv
from google.genai._common import set_value_by_path as setv

from . import types

logger = logging.getLogger('google_genai.endpoints')


def _RequestResponseLoggingConfig_to_mldev(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}
  if getv(from_object, ['default_enabled']) is not None:
    raise ValueError(
        'default_enabled parameter is not supported in Gemini API.'
    )

  return to_object


def _RequestResponseLoggingConfig_to_vertex(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}
  if getv(from_object, ['default_enabled']) is not None:
    setv(to_object, ['defaultEnabled'], getv(from_object, ['default_enabled']))

  return to_object


def _EndpointConfig_to_mldev(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}

  if getv(from_object, ['request_response_logging_config']) is not None:
    raise ValueError(
        'request_response_logging_config parameter is not supported in Gemini'
        ' API.'
    )

  return to_object


def _EndpointConfig_to_vertex(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}

  if getv(from_object, ['request_response_logging_config']) is not None:
    setv(
        to_object,
        ['publisher_model_config', 'request_response_logging_config'],
        _RequestResponseLoggingConfig_to_vertex(
            api_client,
            getv(from_object, ['request_response_logging_config']),
            to_object,
        ),
    )

  return to_object


def _SetEndpointConfigParameters_to_mldev(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}
  if getv(from_object, ['name']) is not None:
    raise ValueError('name parameter is not supported in Gemini API.')

  if getv(from_object, ['config']) is not None:
    raise ValueError('config parameter is not supported in Gemini API.')

  return to_object


def _SetEndpointConfigParameters_to_vertex(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}
  if getv(from_object, ['name']) is not None:
    setv(to_object, ['_url', 'name'], getv(from_object, ['name']))

  if getv(from_object, ['config']) is not None:
    setv(
        to_object,
        ['config'],
        _EndpointConfig_to_vertex(
            api_client, getv(from_object, ['config']), to_object
        ),
    )

  return to_object


def _Operation_from_mldev(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}
  if getv(from_object, ['name']) is not None:
    setv(to_object, ['name'], getv(from_object, ['name']))

  if getv(from_object, ['metadata']) is not None:
    setv(to_object, ['metadata'], getv(from_object, ['metadata']))

  if getv(from_object, ['done']) is not None:
    setv(to_object, ['done'], getv(from_object, ['done']))

  if getv(from_object, ['error']) is not None:
    setv(to_object, ['error'], getv(from_object, ['error']))

  if getv(from_object, ['response']) is not None:
    setv(to_object, ['response'], getv(from_object, ['response']))

  return to_object


def _Operation_from_vertex(
    api_client: BaseApiClient,
    from_object: Union[dict, object],
    parent_object: Optional[dict] = None,
) -> dict:
  to_object: dict[str, Any] = {}
  if getv(from_object, ['name']) is not None:
    setv(to_object, ['name'], getv(from_object, ['name']))

  if getv(from_object, ['metadata']) is not None:
    setv(to_object, ['metadata'], getv(from_object, ['metadata']))

  if getv(from_object, ['done']) is not None:
    setv(to_object, ['done'], getv(from_object, ['done']))

  if getv(from_object, ['error']) is not None:
    setv(to_object, ['error'], getv(from_object, ['error']))

  if getv(from_object, ['response']) is not None:
    setv(to_object, ['response'], getv(from_object, ['response']))

  return to_object


class Endpoints(_api_module.BaseModule):

  def set_endpoint_config(
      self, *, name: str, config: Optional[types.EndpointConfigOrDict] = None
  ) -> types.Operation:
    """Sets a model"""

    parameter_model = types._SetEndpointConfigParameters(
        name=name,
        config=config,
    )

    request_url_dict: Optional[dict[str, str]]
    if not self._api_client.vertexai:
      raise ValueError('This method is only supported in the Vertex AI client.')
    else:
      request_dict = _SetEndpointConfigParameters_to_vertex(
          self._api_client, parameter_model
      )
      request_url_dict = request_dict.get('_url')
      if request_url_dict:
        path = '{name}'.format_map(request_url_dict)
      else:
        path = '{name}'

    query_params = request_dict.get('_query')
    if query_params:
      path = f'{path}?{urlencode(query_params)}'
    # TODO: remove the hack that pops config.
    request_dict.pop('config', None)

    http_options: Optional[types.HttpOptionsOrDict] = None
    if isinstance(config, dict):
      http_options = config.get('http_options', None)
    elif hasattr(config, 'http_options') and config is not None:
      http_options = config.http_options

    request_dict = _common.convert_to_dict(request_dict)
    request_dict = _common.encode_unserializable_types(request_dict)

    response_dict = self._api_client.request(
        'get', path, request_dict, http_options
    )

    if self._api_client.vertexai:
      response_dict = _Operation_from_vertex(self._api_client, response_dict)
    else:
      response_dict = _Operation_from_mldev(self._api_client, response_dict)

    return_value = types.Operation._from_response(
        response=response_dict, kwargs=parameter_model.model_dump()
    )
    self._api_client._verify_response(return_value)
    return return_value


class AsyncEndpoints(_api_module.BaseModule):

  async def set_endpoint_config(
      self, *, name: str, config: Optional[types.EndpointConfigOrDict] = None
  ) -> types.Operation:
    """Sets a model"""

    parameter_model = types._SetEndpointConfigParameters(
        name=name,
        config=config,
    )

    request_url_dict: Optional[dict[str, str]]
    if not self._api_client.vertexai:
      raise ValueError('This method is only supported in the Vertex AI client.')
    else:
      request_dict = _SetEndpointConfigParameters_to_vertex(
          self._api_client, parameter_model
      )
      request_url_dict = request_dict.get('_url')
      if request_url_dict:
        path = '{name}'.format_map(request_url_dict)
      else:
        path = '{name}'

    query_params = request_dict.get('_query')
    if query_params:
      path = f'{path}?{urlencode(query_params)}'
    # TODO: remove the hack that pops config.
    request_dict.pop('config', None)

    http_options: Optional[types.HttpOptionsOrDict] = None
    if isinstance(config, dict):
      http_options = config.get('http_options', None)
    elif hasattr(config, 'http_options') and config is not None:
      http_options = config.http_options

    request_dict = _common.convert_to_dict(request_dict)
    request_dict = _common.encode_unserializable_types(request_dict)

    response_dict = await self._api_client.async_request(
        'get', path, request_dict, http_options
    )

    if self._api_client.vertexai:
      response_dict = _Operation_from_vertex(self._api_client, response_dict)
    else:
      response_dict = _Operation_from_mldev(self._api_client, response_dict)

    return_value = types.Operation._from_response(
        response=response_dict, kwargs=parameter_model.model_dump()
    )
    self._api_client._verify_response(return_value)
    return return_value


__module_sync_client__ = Endpoints
